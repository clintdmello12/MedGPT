{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Inference with Fine-Tuned BLIP VQA Model\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "**Purpose:** This notebook demonstrates how to perform inference using the fine-tuned BLIP Visual Question Answering (VQA) model. The fine-tuning process (presumably done in a separate notebook like `MedGPT_Finetuning.ipynb`) saved PEFT LoRA adapters, and this notebook loads them to answer questions based on user-provided images.\n",
    "\n",
    "**Functionality:**\n",
    "- Loads the pre-trained BLIP model and applies the fine-tuned PEFT LoRA adapters from the `Model/blip-saved-model/` directory.\n",
    "- Provides interactive widgets for users to:\n",
    "  - Upload an image.\n",
    "  - Type a question related to the uploaded image.\n",
    "- Upon image upload, the notebook processes the image and question, feeds them to the fine-tuned model, and displays the model's generated answer.\n",
    "\n",
    "**Key Libraries:**\n",
    "- `transformers`: For loading the BLIP model (`BlipForQuestionAnswering`) and its processor (`BlipProcessor`).\n",
    "- `peft` (Hugging Face PEFT): For loading the LoRA adapter configuration (`PeftConfig`) and potentially applying adapter weights (`PeftModel`) if not already merged.\n",
    "- `PIL` (Pillow): For opening and handling images.\n",
    "- `ipywidgets`: To create interactive file upload and text input widgets in the Jupyter environment.\n",
    "- `IPython.display`: To display images and widgets within the notebook.\n",
    "- `torch`: Core PyTorch library.\n",
    "\n",
    "**Prerequisite Model:**\n",
    "This notebook expects the fine-tuned BLIP model components to be present in the `Model/blip-saved-model/` directory. Specifically, it relies on:\n",
    "- `adapter_config.json`: The PEFT LoRA adapter configuration file.\n",
    "- `adapter_model.safetensors` (or `adapter_model.bin`): The saved PEFT LoRA adapter weights.\n",
    "- Other base model files that `BlipForQuestionAnswering.from_pretrained()` might look for if loading a full saved model. If only adapters were saved, the base model will be downloaded by Hugging Face transformers and then adapters applied."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Imports\n",
    "\n",
    "This cell imports all the necessary libraries:\n",
    "- `BlipProcessor`, `BlipForQuestionAnswering` from `transformers` for the VQA model and its preprocessing.\n",
    "- `requests` (though not explicitly used in the main flow here, often included for fetching data).\n",
    "- `Image` from `PIL` for image manipulation.\n",
    "- `json`, `os`, `csv` for general utility and data handling (though `csv` and `json` are not directly used in this specific inference script's main flow).\n",
    "- `logging` for setting up logging (not actively used in this script).\n",
    "- `tqdm` for progress bars (not actively used in this script's interactive flow).\n",
    "- `torch` for PyTorch functionalities.\n",
    "- `PeftModel`, `PeftConfig` from `peft` for loading PEFT adapters (LoRA in this case).\n",
    "- `io` for handling byte streams (used with image upload).\n",
    "- `display` from `IPython.display` for showing images and widgets in the notebook.\n",
    "- `widgets` from `ipywidgets` for creating the interactive UI components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90a61ce-2d92-435a-b4b4-636921b0ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json, os, csv\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "import io\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. PEFT Configuration Loading\n",
    "\n",
    "This cell loads the PEFT configuration from the directory where the fine-tuned adapter model was saved.\n",
    "- `peft_model_id = \"Model/blip-saved-model\"`: Specifies the path to the saved PEFT adapters and configuration.\n",
    "- `config = PeftConfig.from_pretrained(peft_model_id)`: Loads the `adapter_config.json` file from the specified directory. This configuration contains information about the base model type and the PEFT setup (e.g., LoRA parameters) that was used during fine-tuning. This `config` object itself isn't directly used in the next cell if that cell loads a model that *already has adapters merged or auto-loads them*, but it's good practice to load it to verify the path and to have it available if one were to load the base model and adapters separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"Model/blip-saved-model\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Loading Model and Processor\n",
    "\n",
    "This cell is responsible for loading the actual model and processor needed for inference:\n",
    "- `processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")`: Loads the standard BLIP processor associated with the `Salesforce/blip-vqa-base` model. This processor handles the text tokenization and image preprocessing steps to prepare the data in the format the BLIP model expects.\n",
    "- `model = BlipForQuestionAnswering.from_pretrained(\"Model/blip-saved-model\").to(\"cuda\")`: This is a key step. It loads the `BlipForQuestionAnswering` model.\n",
    "    - Instead of loading the base `Salesforce/blip-vqa-base` model, it directly loads from `\"Model/blip-saved-model\"`.\n",
    "    - The Hugging Face `from_pretrained` method is intelligent: if it finds PEFT adapter files (like `adapter_config.json` and `adapter_model.safetensors`) in the specified directory alongside base model files (or if the base model is specified in `adapter_config.json`), it will automatically load the base model and apply the PEFT adapters. This results in the fine-tuned version of the model being loaded.\n",
    "    - `.to(\"cuda\")` moves the loaded model to the GPU for faster inference, assuming a CUDA-enabled GPU is available.\n",
    "\n",
    "The `stderr` output showing \"Some weights of BlipForQuestionAnswering were not initialized...\" and listing many specific layers is often normal when loading a model that has been fine-tuned with PEFT, especially if only the adapters were saved and are being applied to a freshly loaded base model. It indicates that the base model weights are loaded, and then the adapter weights are applied on top. The listed weights are typically those *not* part of the LoRA adapters (i.e., the original pre-trained weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36e568d-5803-457f-9597-2c789af5806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForQuestionAnswering were not initialized from the model checkpoint at Model/blip-saved-model and are newly initialized: ['text_encoder.encoder.layer.5.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.3.attention.self.value.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.8.output.LayerNorm.bias', 'vision_model.encoder.layers.10.self_attn.projection.bias', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'vision_model.encoder.layers.4.self_attn.qkv.weight', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.self.value.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_encoder.encoder.layer.7.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.intermediate.dense.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'vision_model.embeddings.class_embedding', 'text_encoder.encoder.layer.8.attention.self.key.weight', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'text_encoder.encoder.layer.5.crossattention.self.value.bias', 'text_decoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.output.dense.weight', 'vision_model.encoder.layers.0.self_attn.qkv.bias', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.projection.bias', 'text_encoder.encoder.layer.8.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'text_decoder.bert.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.attention.self.key.weight', 'text_decoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.self.key.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.self.value.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'text_decoder.bert.embeddings.word_embeddings.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.6.crossattention.self.query.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'text_decoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_encoder.encoder.layer.3.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'vision_model.encoder.layers.2.self_attn.projection.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder.cls.predictions.decoder.weight', 'text_decoder.bert.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.1.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.weight', 'vision_model.encoder.layers.6.self_attn.projection.weight', 'vision_model.encoder.layers.6.self_attn.qkv.bias', 'vision_model.encoder.layers.7.self_attn.qkv.bias', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_decoder.bert.encoder.layer.7.attention.self.value.bias', 'text_encoder.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.8.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.weight', 'vision_model.post_layernorm.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'text_encoder.encoder.layer.1.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.weight', 'vision_model.encoder.layers.1.self_attn.projection.bias', 'text_decoder.bert.embeddings.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.weight', 'vision_model.post_layernorm.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_decoder.cls.predictions.transform.dense.weight', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'vision_model.encoder.layers.5.self_attn.qkv.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.self.value.weight', 'text_decoder.bert.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.crossattention.self.key.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.attention.self.key.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.attention.self.value.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'vision_model.encoder.layers.0.self_attn.projection.bias', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_encoder.encoder.layer.4.crossattention.self.key.bias', 'vision_model.encoder.layers.9.self_attn.qkv.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.7.attention.self.query.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.embeddings.patch_embedding.bias', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_decoder.bert.encoder.layer.0.attention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.weight', 'vision_model.encoder.layers.7.self_attn.qkv.weight', 'text_decoder.bert.encoder.layer.9.attention.self.value.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.0.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'text_decoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'text_encoder.encoder.layer.0.crossattention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'vision_model.encoder.layers.9.self_attn.projection.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder.encoder.layer.4.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.4.attention.self.value.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.7.crossattention.self.query.bias', 'vision_model.encoder.layers.5.self_attn.projection.bias', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'vision_model.encoder.layers.0.self_attn.projection.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.self.key.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'text_decoder.bert.encoder.layer.9.attention.self.value.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'text_decoder.bert.encoder.layer.11.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.8.output.dense.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder.encoder.layer.2.crossattention.self.query.bias', 'vision_model.encoder.layers.8.self_attn.projection.bias', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.7.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.bias', 'vision_model.encoder.layers.1.self_attn.qkv.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'text_encoder.embeddings.LayerNorm.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.bias', 'vision_model.encoder.layers.4.self_attn.qkv.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.1.output.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_encoder.embeddings.word_embeddings.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.output.dense.bias', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'text_decoder.bert.encoder.layer.9.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.weight', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.bias', 'text_decoder.cls.predictions.transform.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_decoder.bert.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.bias', 'vision_model.encoder.layers.0.self_attn.qkv.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.3.crossattention.self.query.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.self.key.weight', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'text_decoder.bert.encoder.layer.11.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.self.key.weight', 'vision_model.encoder.layers.3.self_attn.qkv.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.2.output.dense.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'text_encoder.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.self.key.bias', 'vision_model.encoder.layers.1.self_attn.qkv.bias', 'text_encoder.encoder.layer.4.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_decoder.bert.embeddings.position_embeddings.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'text_decoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.1.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.weight', 'text_encoder.encoder.layer.2.crossattention.self.query.weight', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.bias', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.crossattention.self.key.bias', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'text_encoder.encoder.layer.3.crossattention.self.value.weight', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.1.crossattention.self.value.weight', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.bias', 'vision_model.encoder.layers.3.self_attn.qkv.weight', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.bias', 'vision_model.encoder.layers.2.self_attn.qkv.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'text_encoder.encoder.layer.7.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_encoder.encoder.layer.4.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.3.crossattention.self.query.weight', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.self.query.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'text_decoder.bert.encoder.layer.6.attention.self.query.weight', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_encoder.encoder.layer.3.crossattention.self.key.bias', 'vision_model.encoder.layers.4.self_attn.projection.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'vision_model.encoder.layers.8.self_attn.qkv.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_decoder.bert.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.output.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'text_encoder.encoder.layer.5.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'text_decoder.bert.encoder.layer.2.attention.self.value.bias', 'vision_model.encoder.layers.9.self_attn.qkv.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'vision_model.encoder.layers.6.self_attn.projection.bias', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.0.crossattention.self.value.weight', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.10.output.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_decoder.bert.encoder.layer.11.attention.self.key.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.6.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.bias', 'vision_model.encoder.layers.3.self_attn.projection.weight', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.bias', 'text_encoder.encoder.layer.6.crossattention.self.value.weight', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'text_decoder.bert.encoder.layer.8.attention.self.value.weight', 'text_decoder.bert.encoder.layer.8.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.projection.weight', 'text_decoder.bert.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.attention.self.key.bias', 'text_decoder.bert.encoder.layer.6.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.output.dense.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.projection.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'text_encoder.encoder.layer.4.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.bias', 'vision_model.encoder.layers.10.self_attn.qkv.bias', 'text_decoder.bert.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'vision_model.encoder.layers.7.self_attn.projection.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'vision_model.embeddings.patch_embedding.weight', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.crossattention.output.dense.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.3.output.dense.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.weight', 'text_decoder.cls.predictions.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.intermediate.dense.weight', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'vision_model.embeddings.position_embedding', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'text_decoder.bert.encoder.layer.9.attention.self.query.weight', 'vision_model.encoder.layers.2.self_attn.qkv.bias', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_encoder.encoder.layer.7.crossattention.self.value.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder.encoder.layer.3.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.5.crossattention.self.query.bias', 'vision_model.encoder.layers.9.self_attn.projection.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.bias', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.attention.self.query.weight', 'text_decoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.3.attention.self.key.weight', 'vision_model.encoder.layers.10.self_attn.projection.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_encoder.embeddings.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Model/blip-saved-model\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Loading PEFT Model (Commented Out)\n",
    "\n",
    "This cell contains code that would explicitly load the PEFT adapters and apply them to a base model. However, it's commented out.\n",
    "\n",
    "```python\n",
    "# model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "# model.eval()\n",
    "# print(\"Peft model loaded\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `PeftModel.from_pretrained(model, peft_model_id, ...)`: This function is used to load LoRA (or other PEFT) adapters from the `peft_model_id` path and apply them to an existing base `model` (which would have been loaded as, e.g., `BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")`).\n",
    "- `model.eval()`: Sets the model to evaluation mode, which is important for inference as it disables layers like dropout.\n",
    "\n",
    "**Reason for being commented out:**\n",
    "This cell is likely commented out because the previous cell (`model = BlipForQuestionAnswering.from_pretrained(\"Model/blip-saved-model\")`) already handles the loading of the fine-tuned model (base + adapters). The Hugging Face `from_pretrained` method can automatically detect and load PEFT adapters if they are present in the specified directory. Therefore, this explicit step of using `PeftModel.from_pretrained` might be redundant if the model saved in `Model/blip-saved-model/` is either a fully merged model or contains the necessary files for `BlipForQuestionAnswering.from_pretrained` to correctly load the adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "# model.eval()\n",
    "#\n",
    "# print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Interactive Inference Function (`process_image_and_question`)\n",
    "\n",
    "This cell defines the core logic for the interactive VQA demo.\n",
    "\n",
    "**Function `process_image_and_question(change)`:**\n",
    "- This function is designed to be called when the value of the `upload_button` widget changes (i.e., when a file is uploaded).\n",
    "- **Image Handling:**\n",
    "  - It retrieves the uploaded image data from the `upload_button.value`.\n",
    "  - `Image.open(io.BytesIO(image_data)).convert(\"RGB\")`: Opens the image from its byte content and ensures it's in RGB format.\n",
    "  - `display(image)`: Shows the uploaded image in the notebook output.\n",
    "- **Question Handling:**\n",
    "  - `question = question_input.value`: Gets the text entered by the user in the `question_input` widget.\n",
    "- **Inference:**\n",
    "  - `encoding = processor(image, question, return_tensors=\"pt\").to(\"cuda:0\", torch.float16)`: \n",
    "    - The BLIP `processor` prepares the image and question for the model.\n",
    "    - `return_tensors=\"pt\"` ensures PyTorch tensors are returned.\n",
    "    - `.to(\"cuda:0\", torch.float16)` moves the input tensors to the GPU and uses `float16` (half-precision) for potentially faster inference if the model supports it and the hardware is suitable. (Note: `model.eval()` should ideally be called before running inference, which is missing in the active path if cell [4] is commented out and cell [3] is used directly for inference. However, `from_pretrained` might set it to eval mode by default for some model types, or training mode might not drastically affect BLIP generation for VQA if batch norm/dropout are not heavily used in generation paths).\n",
    "  - `out = model.generate(**encoding)`: The model generates an answer sequence based on the processed image and question. `**encoding` unpacks the dictionary of inputs.\n",
    "  - `generated_text = processor.decode(out[0], skip_special_tokens=True)`: The generated output tokens (from `out[0]`) are decoded back into human-readable text by the `processor`. `skip_special_tokens=True` removes any special tokens like padding or end-of-sequence tokens.\n",
    "- **Display Answer:**\n",
    "  - `print(\"Answer:\", generated_text)`: Prints the model's answer.\n",
    "\n",
    "**Widget Setup:**\n",
    "- `upload_button = widgets.FileUpload(...)`: Creates a file upload button. `accept='image/*'` restricts uploads to image files, and `multiple=False` allows only one file at a time.\n",
    "- `question_input = widgets.Text(...)`: Creates a text input field for the user to type their question.\n",
    "\n",
    "**Event Handling:**\n",
    "- `upload_button.observe(process_image_and_question, names='value')`: This is crucial. It links the `process_image_and_question` function to the `upload_button`. Whenever a file is successfully uploaded (i.e., the `value` of the widget changes), the function will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4884a3-23b2-465a-91ab-96efe99b8818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09caecabe02d4b0c86ae227a3d38fde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='image/*', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eaebf90c5f04ecb975b7cd364e144ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Question:', placeholder='Type your question here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import io\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Assuming 'processor' and 'model' are already defined and loaded\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def process_image_and_question(change):\n",
    "    if upload_button.value:\n",
    "        # Get the image file\n",
    "        # Correctly access the uploaded file's content\n",
    "        # For single file upload, upload_button.value is a tuple, get the first item\n",
    "        if isinstance(upload_button.value, tuple) and len(upload_button.value) > 0:\n",
    "            uploaded_file_dict = upload_button.value[0] # Get the dict for the first file\n",
    "            image_data = uploaded_file_dict['content']\n",
    "        elif isinstance(upload_button.value, dict): # Fallback for older ipywidgets or different environments\n",
    "             uploaded_file = next(iter(upload_button.value.values()))\n",
    "             image_data = uploaded_file['content']\n",
    "        else:\n",
    "            print(\"Could not retrieve uploaded file data.\")\n",
    "            return\n",
    "        \n",
    "        image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
    "\n",
    "        # Display the uploaded image\n",
    "        display(image)\n",
    "\n",
    "        # Read the question from the text widget\n",
    "        question = question_input.value\n",
    "\n",
    "        # Prepare inputs\n",
    "        # Ensure inputs are on the same device as the model and correct dtype\n",
    "        inputs = processor(images=image, text=question, return_tensors=\"pt\").to(model.device, dtype=torch.float16 if hasattr(model, 'config') and model.config.torch_dtype == torch.float16 else torch.float32)\n",
    "\n",
    "        # Generate output from the model\n",
    "        with torch.no_grad(): # Ensure no gradients are computed during inference\n",
    "            out = model.generate(**inputs)\n",
    "        generated_text = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        # Display the generated text\n",
    "        print(\"Answer:\", generated_text)\n",
    "        \n",
    "        # Clear the upload button value to allow re-triggering with the same file if needed\n",
    "        upload_button.value.clear()\n",
    "        # Resetting the FileUpload widget properly is tricky; this helps but might not be perfect\n",
    "        # A more robust way is to re-create the widget if re-uploading the exact same file and name is needed\n",
    "\n",
    "# Create widgets\n",
    "upload_button = widgets.FileUpload(\n",
    "    accept='image/*',  # Accept image/* means it can accept any type of image\n",
    "    multiple=False,  # Allow only single file upload\n",
    "    description='Upload Image'\n",
    ")\n",
    "\n",
    "question_input = widgets.Text(\n",
    "    value='What is in this image?', # Default question\n",
    "    description='Question:',\n",
    "    placeholder='Type your question here...',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Set up event to trigger on file upload\n",
    "# Using 'data' instead of 'value' might be more reliable for observing changes after clearing\n",
    "upload_button.observe(process_image_and_question, names='data') # Changed to 'data' for potentially better re-trigger\n",
    "\n",
    "# Display widgets\n",
    "display(upload_button, question_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Displaying Widgets\n",
    "\n",
    "The final active cell in the notebook:\n",
    "`display(upload_button, question_input)`\n",
    "\n",
    "This line uses `IPython.display.display` to render the interactive `FileUpload` button and the `Text` input field in the notebook's output area. The user can now interact with these widgets to upload an image and ask a question. Once an image is uploaded, the `process_image_and_question` function (observed by the upload button) will trigger and perform the VQA inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
